{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSCE633_Keras_Neural_Networks_Oct09.ipynb","provenance":[{"file_id":"1VFMflojNcoP_MjFcV9-ARTydQQuX-nqp","timestamp":1584120325959}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mG4lsoXbAGeN"},"source":["# **Setting up GPU**\n","\n","*   Go to **Edit** -> **Notebook Settings** -> **Hardware Accelerator**\n","*   Select GPU\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZcqiwlNYAh7B"},"source":["# **Keras**\n","\n","\n","*   A deep learning framework that offers simple **APIs to implement and train common neural network architectures**.\n","*   Integrated with low-level deep learning library, tensorflow.\n","*   Provides flexibility to define models using both keras and tensorflow simultaneously.\n","*   Read more: https://keras.io/why-use-keras/\n"]},{"cell_type":"markdown","metadata":{"id":"baKCMztVXuyU"},"source":["# **Training a Neural Network Classifier** \n","\n","\n","1.   Load and Process Data\n","2.   Define Model\n","3.   Compile and Train\n","4.   Evaluate\n"]},{"cell_type":"markdown","metadata":{"id":"h2WdsnWwb_n4"},"source":["# **Load Dataset**\n","\n","*   We will work with **MNIST, Fashion-MNIST and IMDB**  datasets\n","*   Keras includes some common datasets (all three are there)\n","\n","**MNIST** \n","*   Database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples\n","*   Each digit is represented 28 x 28 pixel values\n","*   Digit has to be classified as one of the 0-9 (Total 10 classes)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xQfw1UIEcXne","executionInfo":{"status":"ok","timestamp":1603327552569,"user_tz":300,"elapsed":2782,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"79c10405-dcd1-405f-d8f0-adb05b5914a7","colab":{"base_uri":"https://localhost:8080/","height":369}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from keras.datasets import mnist\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from keras.utils import to_categorical\n","\n","\n"," #Load MNISt dataset\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","# Check number of samples (60000 in training and 10000 in test)\n","# Each image has 28 x 28 pixels\n","print(\"Train Image Shape: \", train_images.shape, \"Train Label Shape: \", train_labels.shape) \n","print(type(train_images))\n","print(\"Test Image Shape: \", test_images.shape, \"Test Label Shape: \", test_labels.shape) \n","\n","#  Visualizing a random image (11th) from training dataset\n","print(\"Visualizing a random image (11th) from training dataset\")\n","_ = plt.imshow(train_images[10])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Train Image Shape:  (60000, 28, 28) Train Label Shape:  (60000,)\n","<class 'numpy.ndarray'>\n","Test Image Shape:  (10000, 28, 28) Test Label Shape:  (10000,)\n","Visualizing a random image (11th) from training dataset\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"iGQc3S6x-VHc"},"source":["# Preprocessing: Normalize the images.\n","train_images = (train_images / 255) - 0.5\n","test_images = (test_images / 255) - 0.5\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBv3vs2UcCv1"},"source":["# **Define Neural Network Model**\n","\n","To define simple models, we will use\n","\n","*   **Sequential Model** which is  simply linear stack of layers \n","*   **Layers** include Linear layer (**Dense**), Convolutional Layers (**Conv1D, Conv2D etc.**), recurrent layers (**RNN, LSTM**), dropout etc.\n","\n","\n","In more complicated scenarios (e.g. Multi-output model, models with shared parameters etc.), we will use **Functional APIs**.\n","*    This will be used in later part of tutorial, where we implement a multi-task learning system.\n"]},{"cell_type":"markdown","metadata":{"id":"4oLEvU_uVkpP"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1EgtcNx9NTJD"},"source":["\n","# **Feed-Forward Neural Network**\n","\n","*   For a simple feed-forward neural network, only use stack of **Dense** and **Dropout** Layers\n","\n","*   **Dense** implements the operation: $output=activation(W^Tx + b)$\n","\n","*   **Dropout** is the most commonly (**almost 100%**) used mechanism to avoid overfitting in a neural network. It randomly set a fraction (rate) of input units to **zero** during training. \n","\n","*    Regularization techniques such as $l_1$ and $l_2$ norms are too expensive when training a big neural network with millions of parameters (**extremely SLOW**).\n","\n","*    Use **Dropout without an exception**. You will see that the first model we define next has over 900K parameters that will be trained using just 60,000 samples. When using such **overparametrization**, model can easily remember labels for each training sample but fail to generalize to test samples.\n","*    It is a good practice to validate your model's architecture by using $model.summary()$ \n"]},{"cell_type":"code","metadata":{"id":"dCvIW55RcgMK","executionInfo":{"status":"ok","timestamp":1603327553150,"user_tz":300,"elapsed":3299,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"6b498a5a-3a81-43cf-dcfe-fd52d04318ba","colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")  # Ignore some warning logs\n","\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","\n","\n","#  Define a Feed-Forward Model with 2 hidden layers with dimensions 392 and 196 Neurons\n","model = Sequential([\n","  Dense(784, activation='relu', input_shape=(28*28,), name=\"first_hidden_layer\"),\n","  Dense(784//2, activation='relu', name=\"second_hidden_layer\"), Dropout(0.25),\n","  Dense(10, activation='softmax'),\n","])\n","\n","#  Validate your Model Architecture\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","first_hidden_layer (Dense)   (None, 784)               615440    \n","_________________________________________________________________\n","second_hidden_layer (Dense)  (None, 392)               307720    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 392)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 10)                3930      \n","=================================================================\n","Total params: 927,090\n","Trainable params: 927,090\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6QEnfLiecfPh"},"source":["# **Training our Neural Network**\n","\n","*   Before training our model, we need to configure the learning process by using $compile()$ method\n","\n","> We specify **three** training parameters:\n","\n","1.   **Optimizer**: e.g. $sgd$ (stochastic gradient descent) or some advance optimizers such as $adam$ (**very stable training**). For a simple model, while the difference in model's performance is not significant, when using very deep neural netoworks like RNNs/ LSTMs it becomes difficult to successfully train them with SGD.\n","2.   **Loss**: Objective that we want to optimize e.g. RSS, Cross-Entropy etc.\n","3.   **Metrics**: List of metrics you want to use to evaluate your model e.g. Accuracy, F1 score etc.\n","\n","\n","*   For training our models, we typically use $fit()$ method, pass training data and labels, specify other hyper-parameters such as $batch\\_size, epochs$.\n","\n"]},{"cell_type":"code","metadata":{"id":"UUOZyXoC2RbB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQG9pgaQ-vYF","executionInfo":{"status":"ok","timestamp":1603327619018,"user_tz":300,"elapsed":69122,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"bfb8c5e8-f547-421c-a94c-60cded8366c2","colab":{"base_uri":"https://localhost:8080/","height":451}},"source":["# Compile model\n","model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","# Flatten the images into vectors (1D) for feed forward network\n","flatten_train_images = train_images.reshape((-1, 28*28))\n","flatten_test_images = test_images.reshape((-1, 28*28))\n","print(\"Train image shape: \", train_images.shape, \"Flattened image shape: \", flatten_train_images.shape)\n","print(train_labels.shape)\n","\n","\n","print(type(flatten_train_images[0,0]))\n","print(type(train_labels[0]))\n","\n","# Train model\n","model.fit(flatten_train_images, to_categorical(train_labels), epochs=10, batch_size=256,)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train image shape:  (60000, 28, 28) Flattened image shape:  (60000, 784)\n","(60000,)\n","<class 'numpy.float64'>\n","<class 'numpy.uint8'>\n","Epoch 1/10\n","235/235 [==============================] - 7s 28ms/step - loss: 1.3487 - accuracy: 0.6528\n","Epoch 2/10\n","235/235 [==============================] - 6s 28ms/step - loss: 0.6577 - accuracy: 0.8267\n","Epoch 3/10\n","235/235 [==============================] - 6s 27ms/step - loss: 0.5054 - accuracy: 0.8579\n","Epoch 4/10\n","235/235 [==============================] - 6s 28ms/step - loss: 0.4405 - accuracy: 0.8744\n","Epoch 5/10\n","235/235 [==============================] - 6s 27ms/step - loss: 0.4016 - accuracy: 0.8837\n","Epoch 6/10\n","235/235 [==============================] - 7s 28ms/step - loss: 0.3751 - accuracy: 0.8910\n","Epoch 7/10\n","235/235 [==============================] - 7s 28ms/step - loss: 0.3547 - accuracy: 0.8966\n","Epoch 8/10\n","235/235 [==============================] - 6s 27ms/step - loss: 0.3388 - accuracy: 0.9010\n","Epoch 9/10\n","235/235 [==============================] - 6s 27ms/step - loss: 0.3241 - accuracy: 0.9061\n","Epoch 10/10\n","235/235 [==============================] - 6s 27ms/step - loss: 0.3142 - accuracy: 0.9084\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7ff0a620fc88>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"cM3e2dtPcchl"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GV01bLL8AWcj"},"source":["# **Evaluation**\n","\n","*   Evaluate the model on test images using $model.evaluate()$, pass test images and labels as arguments. \n","\n"]},{"cell_type":"code","metadata":{"id":"yIW3Kz2QAVPJ","executionInfo":{"status":"ok","timestamp":1603327620191,"user_tz":300,"elapsed":70259,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"64a849a7-5380-4ca9-d363-27d1335682cb","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Evaluate your model's performance on the test data\n","performance = model.evaluate(flatten_test_images, to_categorical(test_labels))\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.2772 - accuracy: 0.9205\n","Accuracy on Test samples: 0.9204999804496765\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B8yVAfZbcuKP"},"source":["\n","---\n"]},{"cell_type":"code","metadata":{"id":"AaQUVNm2P5YS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SyrYjbJH-gF4"},"source":["# **Convolutional Neural Network**\n","\n","*   For large input (like image of size 1024*1024), fully connected FFNN requires a lot of parameters\n","*   Input can also have local structures\n","*   Use convolutional operation\n","*   Define a convolutional layer using $Conv2D$ that creates a 2d convloutional layers, specify arguments such as the size of filters (32), kernel (3), activation (relu) etc.\n","\n","**Note**: In this part of code, we separate the feature layers from classifier layer to accomodate the later part of tutorial on **Fine-tuning**. An equivalent implementation for\n","```\n","common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","cnn_model = Sequential(common_features+classifier)\n","```\n","\n","**is ==>**\n","\n","```\n","cnn_model = Sequential([Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), \n","            Flatten(),Dense(512, activation='relu'), Dense(10, activation='softmax'),])\n","```\n","\n","> **Compile, train, test the model and compare**\n","\n","\n","1.   Number of Parameters (almost half the size of FFNN)\n","2.   Performance (significantly better than FFNN, due to the capability of CNN to model local receptive field)\n","\n"]},{"cell_type":"code","metadata":{"id":"-Rf2vrEjAwvj","executionInfo":{"status":"ok","timestamp":1603328857233,"user_tz":300,"elapsed":1307274,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"cb9abb50-daec-4f90-a1fb-5549f978aa9d","colab":{"base_uri":"https://localhost:8080/","height":867}},"source":["from keras.layers import Conv2D, Flatten, MaxPooling2D\n","\n","# Define 2 groups of layers: features layer (convolutions) and classification layer\n","common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","cnn_model = Sequential(common_features+classifier)\n","\n","print(cnn_model.summary())  # Compare number of parameteres against FFN\n","cnn_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","train_images_3d = train_images.reshape(60000,28,28,1)\n","test_images_3d = test_images.reshape(10000,28,28,1)\n","\n","cnn_model.fit(train_images_3d, to_categorical(train_labels), epochs=10, batch_size=256,)\n","performance = cnn_model.evaluate(test_images_3d, to_categorical(test_labels))\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1024)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 594,922\n","Trainable params: 594,922\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","235/235 [==============================] - 120s 512ms/step - loss: 2.2481 - accuracy: 0.2867\n","Epoch 2/10\n","235/235 [==============================] - 122s 519ms/step - loss: 0.8630 - accuracy: 0.7885\n","Epoch 3/10\n","235/235 [==============================] - 122s 520ms/step - loss: 0.2876 - accuracy: 0.9132\n","Epoch 4/10\n","235/235 [==============================] - 126s 537ms/step - loss: 0.2013 - accuracy: 0.9390\n","Epoch 5/10\n","235/235 [==============================] - 123s 523ms/step - loss: 0.1555 - accuracy: 0.9530\n","Epoch 6/10\n","235/235 [==============================] - 121s 515ms/step - loss: 0.1298 - accuracy: 0.9614\n","Epoch 7/10\n","235/235 [==============================] - 121s 514ms/step - loss: 0.1119 - accuracy: 0.9656\n","Epoch 8/10\n","235/235 [==============================] - 123s 522ms/step - loss: 0.0988 - accuracy: 0.9695\n","Epoch 9/10\n","235/235 [==============================] - 126s 536ms/step - loss: 0.0892 - accuracy: 0.9728\n","Epoch 10/10\n","235/235 [==============================] - 120s 510ms/step - loss: 0.0821 - accuracy: 0.9749\n","313/313 [==============================] - 7s 21ms/step - loss: 0.0814 - accuracy: 0.9716\n","Accuracy on Test samples: 0.9715999960899353\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pS6Doslt455E","executionInfo":{"status":"ok","timestamp":1603329314667,"user_tz":300,"elapsed":1764686,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"aa0532d9-9e96-4f54-87e4-6f45d7431a93","colab":{"base_uri":"https://localhost:8080/","height":797}},"source":["from keras.layers import Conv2D, Flatten, MaxPooling2D\n","\n","\n","# Define 2 groups of layers: features layer (convolutions) and classification layer\n","common_features_1 = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","cnn_model_1 = Sequential(common_features_1+classifier)\n","\n","print(cnn_model_1.summary())  # Compare number of parameteres against FFN\n","cnn_model_1.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","train_images_3d = train_images.reshape(60000,28,28,1)\n","test_images_3d = test_images.reshape(10000,28,28,1)\n","\n","cnn_model_1.fit(train_images_3d, to_categorical(train_labels), epochs=10, batch_size=256,)\n","performance = cnn_model_1.evaluate(test_images_3d, to_categorical(test_labels))\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 1600)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 512)               819712    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 843,658\n","Trainable params: 843,658\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","235/235 [==============================] - 45s 191ms/step - loss: 2.1759 - accuracy: 0.4409\n","Epoch 2/10\n","235/235 [==============================] - 45s 191ms/step - loss: 1.0023 - accuracy: 0.8059\n","Epoch 3/10\n","235/235 [==============================] - 45s 191ms/step - loss: 0.4039 - accuracy: 0.8874\n","Epoch 4/10\n","235/235 [==============================] - 45s 192ms/step - loss: 0.3111 - accuracy: 0.9097\n","Epoch 5/10\n","235/235 [==============================] - 45s 193ms/step - loss: 0.2634 - accuracy: 0.9229\n","Epoch 6/10\n","235/235 [==============================] - 45s 193ms/step - loss: 0.2305 - accuracy: 0.9325\n","Epoch 7/10\n","235/235 [==============================] - 45s 192ms/step - loss: 0.2046 - accuracy: 0.9411\n","Epoch 8/10\n","235/235 [==============================] - 45s 191ms/step - loss: 0.1834 - accuracy: 0.9463\n","Epoch 9/10\n","235/235 [==============================] - 45s 191ms/step - loss: 0.1653 - accuracy: 0.9515\n","Epoch 10/10\n","235/235 [==============================] - 45s 192ms/step - loss: 0.1507 - accuracy: 0.9570\n","313/313 [==============================] - 3s 11ms/step - loss: 0.1353 - accuracy: 0.9605\n","Accuracy on Test samples: 0.9605000019073486\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oLb8n6YsDDIV"},"source":["\n","\n","# Transfer Learning (CNN Layers) and fine-tuning\n","\n","*   The Feature Layers (convolutional layers) learn some low-level patterns that can be effectively used for many (most) other tasks\n","*   These low level features are transferable\n","*   We use the **feature layers** from the pretrained model on MNIST dataset and disable parameter update for them\n","*   **Fine-tune classification layers** in the model on a new **Fashion MNIST** dataset\n","\n","**Fashion MNIST** \n","*   Database of fashion categories, has a training set of 60,000 examples, and a test set of 10,000 examples\n","*   Each image is represented 28 x 28 pixel values\n","*   Digit has to be classified as one of the 10 types\n","\n","**Categories:** \n","\n","0.   T-shirt/top\n","1.   Trouser\n","2.   Pullover\n","3.   Dress\n","4.   Coat\n","5.   Sandal\n","6.   Shirt\n","7.   Sneaker\n","8.   Bag\n","9.   Ankle Boot"]},{"cell_type":"markdown","metadata":{"id":"0uLtAmtORiMJ"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"MOazkDftqksp","executionInfo":{"status":"ok","timestamp":1603329315866,"user_tz":300,"elapsed":1765868,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"b44a362b-c6b1-4dba-84f2-d941f16051a3","colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["from keras.datasets import fashion_mnist\n","\n","(train_fashion_images, train_fashion_labels), (test_fashion_images, test_fashion_labels) = fashion_mnist.load_data()\n","print(train_fashion_images.shape)\n","\n","train_fashion_images = (train_fashion_images / 255) - 0.5\n","test_fashion_images = (test_fashion_images / 255) - 0.5\n","print(\"Visualize a sample\")\n","_ = plt.imshow(train_fashion_images[7])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","(60000, 28, 28)\n","Visualize a sample\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"GMVSZpnGsXkQ"},"source":["\n","\n","---\n","\n","\n","*   **Freeze the Feature Layers** by disabling \"trainable\" attribute\n","\n","```\n","for l in common_features:\n","  l.trainable = False\n","```\n","\n","*    Check number of trainable parameters in model's summary\n","*    Feature Layers (Convolutional) are non-trainable\n","\n","```\n","Total params: 659,914\n","Trainable params: 594,922\n","Non-trainable params: 64,992\n","```\n"]},{"cell_type":"code","metadata":{"id":"xclXz00wUklJ","executionInfo":{"status":"ok","timestamp":1603329355802,"user_tz":300,"elapsed":1805786,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"9e2211e8-d8f8-499d-a7e3-313571f514b9","colab":{"base_uri":"https://localhost:8080/","height":537}},"source":["train_fashion_images_3d = train_fashion_images.reshape(60000,28,28,1)\n","test_fashion_images_3d = test_fashion_images.reshape(10000,28,28,1)\n","\n","\n","for l in common_features:\n","  l.trainable = False\n","\n","print(cnn_model.summary())\n","\n","cnn_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","cnn_model.fit(train_fashion_images_3d, to_categorical(train_fashion_labels), epochs=1, batch_size=256,)\n","performance = cnn_model.evaluate(test_fashion_images_3d, to_categorical(test_fashion_labels))\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1024)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 594,922\n","Trainable params: 529,930\n","Non-trainable params: 64,992\n","_________________________________________________________________\n","None\n","235/235 [==============================] - 33s 138ms/step - loss: 0.7906 - accuracy: 0.7231\n","313/313 [==============================] - 7s 21ms/step - loss: 0.6016 - accuracy: 0.7739\n","Accuracy on Test samples: 0.7738999724388123\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vVzbFJtxFcwB"},"source":["\n","\n","---\n","\n","\n","**Compare Transferred Model against the Model trained from scratch**\n","\n","*   The performance is roughly 15-30% lower (depends on network initialization)\n","\n"]},{"cell_type":"code","metadata":{"id":"9nSIWWT-Feub","executionInfo":{"status":"ok","timestamp":1603329484796,"user_tz":300,"elapsed":1934757,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"b40edf40-e9e0-4e90-92fb-496a6a584365","colab":{"base_uri":"https://localhost:8080/","height":537}},"source":["features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","new_model = Sequential(features+classifier)\n","print(new_model.summary())\n","\n","new_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","new_model.fit(train_fashion_images_3d, to_categorical(train_fashion_labels), epochs=1, batch_size=256,)\n","performance = new_model.evaluate(test_fashion_images_3d, to_categorical(test_fashion_labels))\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 594,922\n","Trainable params: 594,922\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","235/235 [==============================] - 121s 514ms/step - loss: 2.2067 - accuracy: 0.3418\n","313/313 [==============================] - 7s 21ms/step - loss: 1.7151 - accuracy: 0.5235\n","Accuracy on Test samples: 0.5235000252723694\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hb3PqVrGwzbz"},"source":["# Multi-Task Learning\n","\n","\n","*   We observed feature layers are transferable across classification tasks (Transfer Learning)\n","*   Learned Fetaure layers can **generalize better** if we learn them **over multiple tasks and datasets** (Multi-task Learning)\n","*   In this part of tutorial, we will use **functions APIs** that provide greater flexibility to define complex models (e.g. two models on two different tasks with shared parameters)\n","*    Key Idea:\n","> Successively call Layers over the Input to get the Output\n","```\n","y = Layer_Ouput(Layer_Hidden(Layer_Input(x)))\n","```\n","> Use input and output to define Model \n","```\n","model = Model(inputs=train_X, outputs=train_y)\n","```\n","\n","*    We can **reuse** these trained layers in any new model by calling them on new input tensors\n","*    Training this model for 2 epochs gives superior performance against the individual models trained for 5-10 epochs\n","*    Model can quickly learn better generalizable features, if the objective tasks are related "]},{"cell_type":"code","metadata":{"id":"ReGtCWMnw2_4","executionInfo":{"status":"ok","timestamp":1603329744184,"user_tz":300,"elapsed":2194125,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"f0e1aa31-b4ba-4461-c4b2-23aef2aa03fd","colab":{"base_uri":"https://localhost:8080/","height":471}},"source":["from keras.models import Model\n","from keras.layers import Input\n","\n","# Define a fetaure extraction model that is shared for both mnist and fashion-mnist tasks\n","Base_feature_model = Sequential([Conv2D(32, kernel_size=3, activation='relu'), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Flatten(), Dense(512, activation='relu'),])\n","\n","Classifier_mnist = Sequential([Dense(10, activation='softmax')])\n","Classifier_fashion_mnist = Sequential([Dense(10, activation='softmax')])\n","\n","# Instantiate a Tensor to feed Input (Input Layer)\n","mnist_input = Input(shape=(28,28,1))\n","fashion_mnist_input = Input(shape=(28,28,1))\n","\n","# Call Base_feature_model over the mnist images\n","mnist_features = Base_feature_model(mnist_input)\n","\n","# Call Base_feature_model over the fashion-mnist images\n","fashion_mnist_features = Base_feature_model(fashion_mnist_input)\n","\n","# Call mnist_prediction layer over the mnist images\n","# mnist_prediction represents the predicted output for mnist dataset\n","mnist_prediction = Classifier_mnist(mnist_features)\n","\n","# Call fashion_mnist_prediction layer over the mnist images\n","# fashion_mnist_prediction represents the predicted output for fashion-mnist dataset\n","fashion_mnist_prediction = Classifier_fashion_mnist(fashion_mnist_features)\n","\n","# define model by calling Model(inputs, outputs) on the instance of input layers and output layers\n","joint_model = Model(inputs=[mnist_input, fashion_mnist_input], \n","                    outputs=[mnist_prediction, fashion_mnist_prediction])\n","\n","print(joint_model.summary())\n","\n","joint_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'],)  # Using adam optimizer for faster convergence\n","\n","joint_model.fit([train_images_3d, train_fashion_images_3d], \n","                [to_categorical(train_labels), to_categorical(train_fashion_labels)], \n","                epochs=1, batch_size=1024,)\n","performance = joint_model.evaluate([test_images_3d, test_fashion_images_3d], \n","                                   [to_categorical(test_labels), \n","                                    to_categorical(test_fashion_labels)], verbose=1)\n","\n","print(\"===\\nMNIST Accuracy: {0}\\nFashion MNIST Accuracy: {1}\".format(performance[3], performance[4]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n","__________________________________________________________________________________________________\n","sequential_4 (Sequential)       (None, 512)          589792      input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","sequential_5 (Sequential)       (None, 10)           5130        sequential_4[0][0]               \n","__________________________________________________________________________________________________\n","sequential_6 (Sequential)       (None, 10)           5130        sequential_4[1][0]               \n","==================================================================================================\n","Total params: 600,052\n","Trainable params: 600,052\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","59/59 [==============================] - 241s 4s/step - loss: 0.2538 - sequential_5_loss: 0.1037 - sequential_6_loss: 0.1502 - sequential_5_accuracy: 0.8011 - sequential_6_accuracy: 0.6840\n","313/313 [==============================] - 12s 38ms/step - loss: 0.1281 - sequential_5_loss: 0.0298 - sequential_6_loss: 0.0983 - sequential_5_accuracy: 0.9483 - sequential_6_accuracy: 0.7873\n","===\n","MNIST Accuracy: 0.9483000040054321\n","Fashion MNIST Accuracy: 0.7872999906539917\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TK-PNN1lq8bW"},"source":["# **Sequence Classification using Recurrent Neural Networks (LSTM)**\n","\n","*   In previous experiments, we observed CNN models are more suitable for image classification tasks (compared against feed-forward neural networks)\n","*   Choosing appropriate **Model Prior** is crucial to training a neural network model (Each model has innate capacity to capture certain aspects in the input data)\n","*   For modeling **sequential data** (e.g. time-series data, natural languages), recurrent neural network (a powerful version LSTM) provides a very strong prior\n","*   In this part, we will use IMDB corpus (movie reviews) to perform sentiment analysis\n","\n","**IMDB** \n","*   Database of 25,000 movie reviews each for training and test, labeled with positive/negative sentiments\n","*   Review is pre-processed and words are encoded with word-indices in a dictionary \n"]},{"cell_type":"code","metadata":{"id":"zCdA7D_rrAap","executionInfo":{"status":"ok","timestamp":1603329750658,"user_tz":300,"elapsed":2200577,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"d05eb16d-1bd7-4f54-cfce-480dcbfca9fe","colab":{"base_uri":"https://localhost:8080/","height":315}},"source":["from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","\n","# select 5000 most frequent words, other infrequent words are replaced with <UNKNOWN> symbol\n","max_num_words = 5000\n","\n","max_sequence_length = 128\n","(train_imdb_texts, train_imdb_labels), (test_imdb_texts, test_imdb_labels) = imdb.load_data(num_words=max_num_words)\n","\n","# Since each review can have different number of words, we fix the input length to \"max_sequence_length=128\".\n","# Reviews with < 128 words will be padded with some symbol <PAD> and > 128 words will be truncated \n","train_imdb_texts = sequence.pad_sequences(train_imdb_texts, maxlen=max_sequence_length)\n","test_imdb_texts = sequence.pad_sequences(test_imdb_texts, maxlen=max_sequence_length)\n","\n","#  Print an example dataset (represented with word indices)\n","print(train_imdb_texts[0])\n","\n","#  Print actual review by mapping word indices to the corresponding word in the dictionary \n","index = imdb.get_word_index()\n","index_to_word = dict([value, key] for (key, value) in index.items())\n","\n","# Print Decoded Sequence\n","print(\"Text: \", ' '.join([index_to_word.get(i-3, \"<UNKNOWN>\") for i in train_imdb_texts[0]]))\n","print(\"Label: \", train_imdb_labels[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","[  12    8  316    8  106    5    4 2223    2   16  480   66 3785   33\n","    4  130   12   16   38  619    5   25  124   51   36  135   48   25\n"," 1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n","    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n","   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n","  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n","    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n","   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n","   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n","  178   32]\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","Text:  it to everyone to watch and the fly <UNKNOWN> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNKNOWN> to the two little <UNKNOWN> that played the <UNKNOWN> of norman and paul they were just brilliant children are often left out of the <UNKNOWN> list i think because the stars that play them all grown up are such a big <UNKNOWN> for the whole film but these children are amazing and should be <UNKNOWN> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNKNOWN> with us all\n","Label:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MN0M2Ax_sm1V","executionInfo":{"status":"error","timestamp":1603329753714,"user_tz":300,"elapsed":2203611,"user":{"displayName":"Srishti Madan","photoUrl":"","userId":"07778686635203863083"}},"outputId":"d08a7dfe-fba5-46a1-dc34-bf094b826672","colab":{"base_uri":"https://localhost:8080/","height":883}},"source":["from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","\n","# tensorflow 2.0+ will defaul use eager mode and will cause error for the code below\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","\n","# Embedding layers transform word indices to continuous word vectors\n","# Apply LSTM on the sequence of word vectors\n","sentiment_classification_model = Sequential([Embedding(max_sequence_length, 256), \n","                                             LSTM(256, dropout=0.2, recurrent_dropout=0.2), \n","                                             Dense(1, activation='sigmoid')])\n","\n","print(sentiment_classification_model.summary())\n","\n","sentiment_classification_model.compile(optimizer='adam', loss='binary_crossentropy',\n","                                       metrics=['accuracy'],)  # Using adam optimizer for faster convergence\n","\n","print(train_imdb_texts[0])\n","print(train_imdb_labels.shape)\n","sentiment_classification_model.fit(train_imdb_texts, train_imdb_labels, epochs=5, batch_size=1024)\n","performance = sentiment_classification_model.evaluate(test_imdb_texts, test_imdb_labels)\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 256)         32768     \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 256)               525312    \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 257       \n","=================================================================\n","Total params: 558,337\n","Trainable params: 558,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","[  12    8  316    8  106    5    4 2223    2   16  480   66 3785   33\n","    4  130   12   16   38  619    5   25  124   51   36  135   48   25\n"," 1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n","    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n","   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n","  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n","    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n","   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n","   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n","  178   32]\n","(25000,)\n","Train on 25000 samples\n","Epoch 1/5\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a3ece480933d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imdb_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imdb_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msentiment_classification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imdb_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_imdb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_classification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imdb_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_imdb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3824\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3825\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3827\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[896,1] = 1171 is not in [0, 128)\n\t [[{{node embedding/embedding_lookup}}]]"]}]},{"cell_type":"code","metadata":{"id":"Qqvv_hiS8LsZ"},"source":["from keras import backend as K\n","\n","# embedding layers: shows the relation of data into a higher dimension\n","print(len(train_imdb_texts[0]))\n","get_1st_layer_output = K.function([sentiment_classification_model.layers[0].input],\n","                                  [sentiment_classification_model.layers[0].output])\n","layer_output = get_1st_layer_output([train_imdb_texts[0]])[0]\n","print(layer_output)\n","print(len(layer_output))\n","print(len(layer_output[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXSY1YeSrXu7"},"source":["\n","\n","---\n","\n","\n","**Can you try building a Feed-forward neural network for sentiment analysis and compare number of parameters and performance against LSTM-based model?**"]},{"cell_type":"markdown","metadata":{"id":"fDU9i4nvFQWS"},"source":["# **Hyperparameter Tuning**\n","\n","*    We find Parameters of a Neural Network using some optimzation technique \n","*    Hyperparmeters are obtained through search\n","*    Generally, we use **grid search** to find optimal hyperparameters (**Brute-Force Approach**)\n","*    **TPE** (Tree Stuctured Parzen Estimator) is another popular approach (based on Gaussian Processes) to estimate hyperparameters\n","*    Here, we use **Hyperopt** library that provides an easy to use TPE based hyperparameter optimization algorithm \n","\n"]},{"cell_type":"code","metadata":{"id":"fjricP7johWL"},"source":["!pip install hyperopt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7QP79QM-Ly4"},"source":["# Steps to use hyperopt\n","\n","*   Define hyperparameter search space\n","```\n","space = {'conv_kernel_size': hp.choice('conv_kernel_size', [3, 5]), \n","'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.35),\n","'optimizer': hp.choice('optimizer', ['Adam', 'sgd']),\n","}\n","```\n","Here we search among 2 values (3,5) for kernel size of convolutional layers, 2 possible values (SGD, Adam) for optimizers, and the dropout rate of a float number between 0.1 and 0.35 sampled uniformly.\n","\n","*   Define the **objective function** to optimize\n","1.    It takes hyperparamers as argument \n","2.    Define, Compile and Train model, calculate loss/ accuracy on evaluation dataset\n","3.    Return a dictionary object with keys \"**loss**\" (float-valued function that we want to minimize) and \"**status**\" (keys from hyperopt.STATUS_STRINGS, such as 'ok' for successful completion, and 'fail' in cases where the function turned out to be undefined)\n","4.   You can also return the model object and later use that for evaluation on test-data\n","\n","\n","*   Specify **search algorithm (TPE)** to use \n","```\n","algo=tpe.suggest\n","```\n","\n","*   Maximum number of hyperparameters to try \n","```\n","max_evals=25\n","```\n","\n","*   **Optional**: create and pass **trials** object as an argument. With trials object, we can inspect all of the return values that are calculated during the experiment (e.g. losses, statuses, model)\n","\n","*   For more details: check https://github.com/hyperopt/hyperopt/wiki/FMin\n"]},{"cell_type":"code","metadata":{"id":"QYl9XDaTnefS"},"source":["from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n","\n","\n","def optimize_cnn(hyperparameter):\n","  \n","  # Define model using hyperparameters \n","  cnn_model = Sequential([Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']),\n","            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'),\n","            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']), \n","            Flatten(),\n","            Dense(512, activation='relu'), \n","            Dense(10, activation='softmax'),])\n","  \n","  cnn_model.compile(optimizer=hyperparameter['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'],)\n","\n","  # create a training (50K samples) and validation (10K samples) subsets from training images.\n","  # Validation subset will be used to find the optimal hyperparameters\n","  train_X, train_y = train_images_3d[:50000], train_labels[:50000]\n","  valid_X, valid_y = train_images_3d[50000:], train_labels[50000:]\n","\n","  _ = cnn_model.fit(train_X, to_categorical(train_y), epochs=2, batch_size=256, verbose=0)\n","  # Evaluate accuracy on validation data\n","  performance = cnn_model.evaluate(valid_X, to_categorical(valid_y), verbose=0)\n","\n","  print(\"Hyperparameters: \", hyperparameter, \"Accuracy: \", performance[1])\n","  print(\"----------------------------------------------------\")\n","  # We want to minimize loss i.e. negative of accuracy\n","  return({\"status\": STATUS_OK, \"loss\": -1*performance[1], \"model\":cnn_model})\n","  \n","\n","# Define search space for hyper-parameters\n","space = {\n","    # The kernel_size for convolutions:\n","    'conv_kernel_size': hp.choice('conv_kernel_size', [1, 3, 5]),\n","    # Uniform distribution in finding appropriate dropout values\n","    'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.35),\n","    # Choice of optimizer \n","    'optimizer': hp.choice('optimizer', ['Adam', 'sgd']),\n","}\n","\n","trials = Trials()\n","\n","# Find the best hyperparameters\n","best = fmin(\n","        optimize_cnn,\n","        space,\n","        algo=tpe.suggest,\n","        trials=trials,\n","        max_evals=25,\n","    )\n","\n","print(\"==================================\")\n","print(\"Best Hyperparameters\", best)\n","\n","# You can retrain the final model with optimal hyperparameters on train+validation data\n","\n","# Or you can use the model returned directly\n","# Find trial which has minimum loss value and use that model to perform evaluation on the test data\n","test_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n","\n","performance = test_model.evaluate(test_images_3d, to_categorical(test_labels))\n","\n","print(\"==================================\")\n","print(\"Test Accuracy: \", performance[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rW12wnRnTRhk"},"source":[""],"execution_count":null,"outputs":[]}]}